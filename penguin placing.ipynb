{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd996670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.nn import Unfold # this is PyTorch's Im2Col function\n",
    "\n",
    "\n",
    "def w_idx2a_idx(Ashape, tshape, i):\n",
    "    n,m = Ashape\n",
    "    h,l = tshape\n",
    "    \n",
    "    n_rows = m - l + 1\n",
    "    return np.array([i // n_rows, i % n_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7e61e",
   "metadata": {},
   "source": [
    "# Hiding a photo within another\n",
    "\n",
    "This code takes a target image and finds the optimal hiding spot for it within another photo. This project was inspired by the [Nature Briefing](https://www.nature.com/nature/articles?type=nature-briefing). \n",
    "\n",
    "For more details, see the related blog post on my [website](https://nmarzz.github.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e54f8da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Import and adjust our target image (The image to hide)\n",
    "\n",
    "## We'll scale down our base and target image for efficiency\n",
    "base_scale = 10\n",
    "target_scale = base_scale\n",
    "\n",
    "# We can slightly resize our penguin image to hide it better\n",
    "penguin_scale = 1\n",
    "penguin_size = np.round(penguin_scale * np.array([40, 80])).astype(int)\n",
    "\n",
    "t_im = Image.open('rockhopper.png')\n",
    "# t_im = t_im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "t_im = t_im.convert('RGBA')\n",
    "t_im = t_im.resize(list(penguin_size)) # Scale the penguin down to hidding size\n",
    "\n",
    "# We'll also try hiding the flipped image\n",
    "t_im_flip = t_im.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "\n",
    "t_im_resized = t_im.resize(np.array(t_im.size) // (target_scale)) # scale down for efficiency\n",
    "t_im_flip_resized = t_im_flip.resize(np.array(t_im.size) // (target_scale))\n",
    "\n",
    "\n",
    "# Form our target matrix for original t\n",
    "t = np.array(t_im_resized).transpose()\n",
    "mask = (np.concatenate([t[-1,:,:].flatten(),t[-1,:,:].flatten(),t[-1,:,:].flatten()]) == 0) # Accounting for Leif's non-rectangularity\n",
    "\n",
    "t = torch.tensor(t[0:3,:,:], dtype = torch.float32)\n",
    "flat_t = t.flatten()\n",
    "t_size = (t.shape[1], t.shape[2])\n",
    "\n",
    "\n",
    "# Form our target matrix for flipped t\n",
    "t_flip = np.array(t_im_flip_resized).transpose()\n",
    "mask_flip = (np.concatenate([t_flip[-1,:,:].flatten(),t_flip[-1,:,:].flatten(),t_flip[-1,:,:].flatten()]) == 0) # Accounting for Leif's non-rectangularity\n",
    "\n",
    "t_flip = torch.tensor(t_flip[0:3,:,:], dtype = torch.float32)\n",
    "flat_t_flip = t_flip.flatten()\n",
    "\n",
    "\n",
    "###### Import and adjust our base Image \n",
    "base = Image.open('sunset.jpeg')\n",
    "base = base.convert('RGBA')\n",
    "base_resized = base.resize(np.array(base.size) // base_scale)\n",
    "\n",
    "# Form our base matrix\n",
    "A = np.array(base_resized).transpose()\n",
    "A = torch.tensor(A, dtype = torch.float32)\n",
    "A = torch.unsqueeze(A,0)\n",
    "A = A[:,0:3,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7526afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate W with im2col\n",
    "unfold = Unfold(kernel_size= t_size)\n",
    "W = unfold(A).squeeze(0).transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f3d41f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance between each patch and our original target \n",
    "difference = W - t.flatten()\n",
    "difference[:,mask] = 0 # Ignore the masked out entries\n",
    "norms = np.linalg.norm(difference, axis = 1)\n",
    "\n",
    "min_norm = np.min(norms)\n",
    "optimal_window_idx = np.argmin(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "49e19e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance between each patch and our original target \n",
    "difference = W - t_flip.flatten()\n",
    "difference[:,mask_flip] = 0 # Ignore the masked out entries\n",
    "norms = np.linalg.norm(difference, axis = 1)\n",
    "\n",
    "min_norm_flip = np.min(norms)\n",
    "optimal_window_idx_flip = np.argmin(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5878be18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better orientation: flipped\n",
      "Optimal index is [220  20]\n"
     ]
    }
   ],
   "source": [
    "# Find the location of the best patch \n",
    "if min_norm <= min_norm_flip:\n",
    "    optimal_index = w_idx2a_idx((A.shape[2],A.shape[3]), (t.shape[1], t.shape[2]) ,optimal_window_idx)\n",
    "    paste_im = t_im\n",
    "    print('Better orientation: original')\n",
    "else:\n",
    "    optimal_index = w_idx2a_idx((A.shape[2],A.shape[3]), (t.shape[1], t.shape[2]) ,optimal_window_idx_flip)\n",
    "    paste_im = t_im_flip    \n",
    "    print('Better orientation: flipped')    \n",
    "print(f'Optimal index is {optimal_index * base_scale}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "871e768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide our target and display!\n",
    "base.paste(paste_im, tuple(optimal_index * base_scale) ,  mask=paste_im) \n",
    "base.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
